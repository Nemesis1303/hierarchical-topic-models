{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b530175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['xtick.labelsize'] = 24\n",
    "plt.rcParams['ytick.labelsize'] = 24\n",
    "plt.rcParams['font.size'] = 22\n",
    "plt.rcParams['axes.titlesize'] = 24\n",
    "plt.rcParams['axes.labelsize'] = 24\n",
    "plt.rcParams['legend.fontsize'] = 24\n",
    "plt.rcParams['lines.markersize'] = 13\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['lines.linewidth'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da7d79",
   "metadata": {},
   "source": [
    "### Paths to HTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b5201a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_models_mallet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m path_models_ctm_s2cs \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/export/usuarios_ml4ds/lbartolome/Datasets/S2CS/models_htm_ctm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m path_models_mallet_s2cs \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/export/usuarios_ml4ds/lbartolome/Datasets/S2CS/models_htm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m path_models \u001b[38;5;241m=\u001b[39m \u001b[43mpath_models_mallet\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_models \u001b[38;5;241m==\u001b[39m path_models_ctm_cordis:\n\u001b[1;32m      7\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcordis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_models_mallet' is not defined"
     ]
    }
   ],
   "source": [
    "path_models_ctm_cordis = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Datasets/CORDIS/models_htm_ctm\")\n",
    "path_models_mallet_cordis = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Datasets/CORDIS/models_htm\")\n",
    "path_models_ctm_s2cs = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Datasets/S2CS/models_htm_ctm\")\n",
    "path_models_mallet_s2cs = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Datasets/S2CS/models_htm\")\n",
    "path_models = path_models_mallet\n",
    "if path_models == path_models_ctm_cordis:\n",
    "    corpus = \"cordis\"\n",
    "    model_type = \"ctm\"\n",
    "elif path_models == path_models_mallet_cordis:\n",
    "    corpus = \"cordis\"\n",
    "    model_type = \"mallet\"\n",
    "elif path_models == path_models_ctm_s2cs:\n",
    "    corpus = \"s2cs\"\n",
    "    model_type = \"ctm\"\n",
    "elif path_models == path_models_mallet_s2cs:\n",
    "    corpus = \"s2cs\"\n",
    "    model_type = \"mallet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6035f0f",
   "metadata": {},
   "source": [
    "### Get root models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for entry in path_models.iterdir():\n",
    "    # check if it is a root model\n",
    "    if \"root\" in entry.as_posix():\n",
    "        # Path to the root model\n",
    "        path = entry\n",
    "        \n",
    "        # Thr and exp_tpc do not apply for the root model\n",
    "        thr = -1\n",
    "        exp_tpc = -1\n",
    "        \n",
    "        # Experiment iteration\n",
    "        iter_ = int(entry.as_posix().split(\"model_\")[1].split(\"_\")[0])\n",
    "        \n",
    "        # Size of the topics\n",
    "        alphas = np.load(path.joinpath('TMmodel/alphas.npy')).tolist()\n",
    "        \n",
    "        # Coherences (CV and NPMI)\n",
    "        cohrs = np.load(path.joinpath('TMmodel/topic_coherence.npy')).tolist()\n",
    "        if len(cohrs) > len(alphas):\n",
    "            cohrs_cv = cohrs[0:len(alphas)]\n",
    "            cohrs_npmi = cohrs[len(alphas):]\n",
    "        elif len(cohrs) == len(alphas):\n",
    "            cohrs_cv = cohrs\n",
    "            cohrs_npmi = [0] * len(alphas)\n",
    "        \n",
    "        # Topics' entropies\n",
    "        entropies = np.load(path.joinpath('TMmodel/topic_entropy.npy')).tolist()\n",
    "        \n",
    "        # Ids of the topics\n",
    "        tpc_ids = np.arange(0,len(alphas),1)\n",
    "        \n",
    "        # Corpus size\n",
    "        if path.joinpath('corpus.txt').is_file():\n",
    "            corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                path.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "            size = len(corpus)\n",
    "        elif path.joinpath('corpus.parquet').is_dir():\n",
    "            dfc = pd.read_parquet(path.joinpath('corpus.parquet'))\n",
    "            size = len(dfc)\n",
    "            \n",
    "        # Create dataframe for the root model\n",
    "        root_tpc_df = pd.DataFrame(\n",
    "            {'iter': [iter_] * len(alphas),\n",
    "             'path': [path] * len(alphas),\n",
    "             'cohrs_cv': cohrs_cv,\n",
    "             'cohrs_npmi': cohrs_npmi,\n",
    "             'entropies': entropies,\n",
    "             'alphas': alphas,\n",
    "             'tpc_ids': tpc_ids,\n",
    "             'thr': [thr] * len(alphas),\n",
    "             'exp_tpc': [exp_tpc] * len(alphas),\n",
    "             'size': [size] * len(alphas),\n",
    "             'tr_tpcs': [0] * len(alphas),\n",
    "            })\n",
    "        \n",
    "        # Append to the list of dataframes to concatenate them\n",
    "        dfs.append(root_tpc_df)\n",
    "df = pd.concat(dfs)\n",
    "df = df.sort_values(by=['iter'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91504a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.iloc[0].path.joinpath('corpus.txt').is_file():\n",
    "    corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                df.iloc[0].path.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "    root_size = len(corpus)\n",
    "elif df.iloc[0].path.joinpath('corpus.parquet').is_dir():\n",
    "    dfc = pd.read_parquet(df.iloc[0].path.joinpath('corpus.parquet'))\n",
    "    root_size = len(dfc)\n",
    "root_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a060aa",
   "metadata": {},
   "source": [
    "### Get submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a816eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iter over each root model (according to its corresponding iteration, iter)\n",
    "concats = [df]\n",
    "not_finished = []\n",
    "for el in df.iter.unique():\n",
    "    path_root = df[df.iter == el].iloc[0].path\n",
    "    for entry in path_root.iterdir():\n",
    "        if entry.joinpath('TMmodel/topic_coherence.npy').is_file():\n",
    "        \n",
    "            if \"ws\" in entry.as_posix():\n",
    "                thr = 0\n",
    "                size = 0\n",
    "            else:\n",
    "                thr = float(entry.as_posix().split(\"thr_\")[1].split(\"_\")[0])\n",
    "                \n",
    "                if entry.joinpath('corpus.txt').is_file():\n",
    "                    corpus = [line.rsplit(' 0 ')[1].strip() for line in open(\n",
    "                                entry.joinpath('corpus.txt'), encoding=\"utf-8\").readlines()]\n",
    "                    size = len(corpus)\n",
    "                elif entry.joinpath('corpus.parquet').is_dir():\n",
    "                    dfc = pd.read_parquet(entry.joinpath('corpus.parquet'))\n",
    "                    size = len(dfc)\n",
    "                size = size * 100 / root_size\n",
    "\n",
    "            # get topic from which the submodel is generated\n",
    "            exp_tpc = int(entry.as_posix().split(\"from_topic_\")[1].split(\"_\")[0])\n",
    "            \n",
    "            \n",
    "            # Size of the topics\n",
    "            alphas = np.load(entry.joinpath('TMmodel/alphas.npy')).tolist()\n",
    "        \n",
    "            # Alphas submodel is the mean of the cohr of its topics\n",
    "            alpha = np.mean(alphas)\n",
    "            \n",
    "            # Coherences (CV and NPMI)\n",
    "            cohrs = np.load(entry.joinpath('TMmodel/topic_coherence.npy')).tolist()\n",
    "            if len(cohrs) > len(alphas):\n",
    "                cohrs_cv = cohrs[0:len(alphas)]\n",
    "                cohrs_npmi = cohrs[len(alphas):]\n",
    "            elif len(cohrs) == len(alphas):\n",
    "                cohrs_cv = cohrs\n",
    "                cohrs_npmi = [0] * len(alphas)\n",
    "            \n",
    "            # cohr submodel is the mean of the cohr of its topics\n",
    "            cohr_cv = np.mean(cohrs_cv)\n",
    "            cohr_npmi = np.mean(cohrs_npmi)\n",
    "            \n",
    "            # Topics' entropies\n",
    "            entropy = np.mean(np.load(entry.joinpath('TMmodel/topic_entropy.npy')).tolist())\n",
    "            \n",
    "            tr_tpcs = int(entry.as_posix().split(\"train_with_\")[1].split(\"_\")[0])\n",
    "            \n",
    "            # add entry of submodel to dataframe\n",
    "            root_tpc_df = pd.DataFrame(\n",
    "            {'iter': [el],\n",
    "             'path': [entry],\n",
    "             'cohrs_cv': [cohr_cv],\n",
    "             'cohrs_npmi': [cohr_npmi],\n",
    "             'entropies': [entropy],\n",
    "             'alphas': [alpha],\n",
    "             'tpc_ids': [exp_tpc],\n",
    "             'thr': [thr],\n",
    "             'exp_tpc': [exp_tpc],\n",
    "             'size': [size],\n",
    "             'tr_tpcs': [tr_tpcs]\n",
    "            })\n",
    "            concats.append(root_tpc_df)\n",
    "        else:\n",
    "            not_finished.append(entry)\n",
    "df = pd.concat(concats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef52215",
   "metadata": {},
   "source": [
    "### Generate graphs for root models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root = df[df.thr==-1]\n",
    "df_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_root.groupby('tpc_ids')[['cohrs_cv', 'cohrs_npmi', 'entropies', 'alphas']].mean()\n",
    "df1 = df1.rename(columns={'cohrs_cv': 'cohrs_cv_mean',\n",
    "                          'cohrs_npmi': 'cohrs_npmi_mean',\n",
    "                          'entropies': 'entropies_mean',\n",
    "                          'alphas': 'alphas_mean',\n",
    "                         })\n",
    "\n",
    "df2 = df_root.groupby('tpc_ids')[['cohrs_cv', 'cohrs_npmi', 'entropies']].var()\n",
    "df2 = df2.rename(columns={'cohrs_cv': 'cohrs_cv_var',\n",
    "                          'cohrs_npmi': 'cohrs_npmi_var',\n",
    "                          'entropies': 'entropies_var',\n",
    "                         })\n",
    "\n",
    "df_root_plot = pd.concat([df1, df2], axis=1, join='inner')\n",
    "\n",
    "df_root_plot['tpc_ids'] = np.arange(0,len(df_root_plot),1)\n",
    "df_root_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize = (55, 10),  dpi=120)\n",
    "y_repr = ['cohrs_cv', 'cohrs_npmi', 'entropies']\n",
    "y_labels = ['Coherence CV', 'Coherence NPMI', 'Entropy']\n",
    "colors = [\"#2D6187\", \"#28ABB9\", \"#387838\"]\n",
    "maxs_mins = [(0.98,1.015),(0.95,1.04),(0.99,1.003)]\n",
    "\n",
    "labels_all = []\n",
    "handles_all = []\n",
    "for yrepr, ylabel, color, max_min, ax in zip(y_repr, y_labels, colors, maxs_mins, axs.flat):\n",
    "    \n",
    "    df_root_plot.plot.bar(\n",
    "        ax=ax,\n",
    "        y=yrepr + \"_mean\",\n",
    "        yerr = yrepr + \"_var\",\n",
    "        label=ylabel, use_index=True, stacked=True,\n",
    "        color = color,\n",
    "        capsize=4)\n",
    "    ax2 = df_root_plot.plot(\n",
    "            ax=ax, \n",
    "            y='alphas_mean', kind = 'line', linewidth=4, label='Size', color=\"#A9294F\", use_index=True, secondary_y=True)\n",
    "\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Topic ID')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_ylim([df_root_plot[yrepr + \"_mean\"].values.min()*max_min[0], df_root_plot[yrepr + \"_mean\"].values.max()*max_min[1]])\n",
    "    ax2.set_ylabel('Size')\n",
    "    ax.get_legend().remove()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels_all += labels\n",
    "    handles_all += handles\n",
    "\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "fig.legend(handles_all+handles2, labels_all+labels2, loc='upper center', ncol=4, bbox_to_anchor=(0.5, 1.03),\n",
    "           frameon=True, shadow=True)\n",
    "\n",
    "save_fig = \"Images/root_model_\" + corpus + \"_\" + model_type + \".png\"\n",
    "fig.savefig(save_fig, dpi='figure', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31066a68",
   "metadata": {},
   "source": [
    "### Get graphs for HTM-WS submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae99bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws = df[df.thr==0]\n",
    "\n",
    "concat = []\n",
    "for el in df_ws.tr_tpcs.unique():\n",
    "    \n",
    "    df1 = df_ws[df_ws.tr_tpcs==el].groupby('exp_tpc')[['cohrs_cv', 'cohrs_npmi', 'entropies']].mean()\n",
    "    df1 = df1.rename(columns={'cohrs_cv': 'cohrs_cv_mean' + \"_\" + str(el),\n",
    "                              'cohrs_npmi': 'cohrs_npmi_mean' + \"_\" + str(el),\n",
    "                              'entropies': 'entropies_mean' + \"_\" + str(el)\n",
    "                             })\n",
    "    concat.append(df1)\n",
    "\n",
    "    df2 = df_ws[df_ws.tr_tpcs==el].groupby('exp_tpc')[['cohrs_cv', 'cohrs_npmi', 'entropies']].var()\n",
    "    df2 = df2.rename(columns={'cohrs_cv': 'cohrs_cv_var' + \"_\" + str(el),\n",
    "                              'cohrs_npmi': 'cohrs_npmi_var' + \"_\" + str(el),\n",
    "                              'entropies': 'entropies_var' + \"_\" + str(el)\n",
    "                             })\n",
    "    concat.append(df2)\n",
    "\n",
    "df_ws_plot = pd.concat(concat, axis=1, join='inner').reset_index()\n",
    "\n",
    "df_ws_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize = (50, 8), dpi=120)\n",
    "y_repr = ['cohrs_cv', 'cohrs_npmi', 'entropies']\n",
    "y_labels = ['Coherence CV', 'Coherence NPMI', 'Entropy']\n",
    "colors = [['#2D6187','#3573A0','#83B3D6'],\n",
    "          ['#28ABB9','#2dc1d0','#8adde6'],\n",
    "          ['#387838', '#82AB82','#ABBEAB']]\n",
    "maxs_mins = [(0.98,1.015),(0.95,1.02),(0.99,1.003)]\n",
    "\n",
    "for yrepr, ylabel, color, max_min, ax in zip(y_repr, y_labels, colors, maxs_mins, axs.flat):\n",
    "    aux = [df_ws_plot[yrepr + \"_var_6\"].values,\n",
    "           df_ws_plot[yrepr + \"_var_8\"].values, \n",
    "           df_ws_plot[yrepr + \"_var_10\"].values]\n",
    "    y_aux = [yrepr + \"_mean_6\", yrepr + \"_mean_8\", yrepr + \"_mean_10\"]\n",
    "    df_ws_plot.plot.bar(\n",
    "        x='exp_tpc',\n",
    "        ax=ax,\n",
    "        y= y_aux,\n",
    "        yerr = aux,\n",
    "        label=['6 tpcs', '8 tpcs', '10 tpcs'],\n",
    "        color=color,\n",
    "        capsize=4)\n",
    "    \n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Topic ID')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_ylim([df_ws_plot[y_aux].values.min()*max_min[0], df_ws_plot[y_aux].values.max()*max_min[1]])\n",
    "    ax.set_xticklabels(ax.get_xticks(), rotation = 0)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n",
    "          frameon=True, shadow=True, ncol=3)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "save_fig = \"Images/htm_ws_\" + corpus + \"_\" + model_type + \".png\"\n",
    "fig.savefig(save_fig, dpi='figure', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937077e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_min = (600*100)/root_size\n",
    "prop_max = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8791ab",
   "metadata": {},
   "source": [
    "### Get graphs for HTM-DS submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds = df[df.thr>0]\n",
    "df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_display = ['cohrs_cv', 'cohrs_npmi', 'entropies']\n",
    "y_labels = ['Coherence CV', 'Coherence NPMI', 'Entropy']\n",
    "colors = [['#2D6187','#3573A0','#83B3D6'],\n",
    "          ['#28ABB9','#2dc1d0','#8adde6'],\n",
    "          ['#387838', '#82AB82','#ABBEAB']]\n",
    "maxs_mins = [(0.98,1.05),(-0.98,1.2),(0.98,1.02)]\n",
    "\n",
    "for metric,label,color,max_min in zip(metrics_display,y_labels,colors,maxs_mins):\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=5, ncols=2, figsize = (50, 50), sharex=True, sharey=True,  dpi=120)\n",
    "\n",
    "    for tpc, ax in zip(sorted(df_ds.exp_tpc.unique()), axs.flat):\n",
    "        aux_df_ds = df_ds[df_ds.exp_tpc==tpc]\n",
    "        concat = []\n",
    "        for el in aux_df_ds.tr_tpcs.unique():\n",
    "\n",
    "            df1 = aux_df_ds[aux_df_ds.tr_tpcs==el].groupby('thr')[[metric,'size']].mean()\n",
    "            df1 = df1.rename(columns={metric: metric + '_mean_' + str(el),\n",
    "                                      'size':  'size_mean' + \"_\" + str(el)})\n",
    "            concat.append(df1)\n",
    "\n",
    "            df2 = aux_df_ds[aux_df_ds.tr_tpcs==el].groupby('thr')[[metric,'size']].var()\n",
    "            df2 = df2.rename(columns={metric: metric + '_var_' + str(el),\n",
    "                                      'size':  'size_var' + \"_\" + str(el)})\n",
    "            concat.append(df2)\n",
    "\n",
    "        aux_df_ds_plot = pd.concat(concat, axis=1, join='inner').reset_index()\n",
    "                \n",
    "        aux = [aux_df_ds_plot[metric + \"_var_6\"].values,\n",
    "               aux_df_ds_plot[metric + \"_var_8\"].values, \n",
    "               aux_df_ds_plot[metric + \"_var_10\"].values]\n",
    "            \n",
    "        y_aux = [metric + \"_mean_6\", metric + \"_mean_8\", metric + \"_mean_10\"]\n",
    "\n",
    "        aux_df_ds_plot.plot.bar(\n",
    "                ax=ax,\n",
    "                x='thr',\n",
    "                y=y_aux,#['cohrs_mean_6','cohrs_mean_8','cohrs_mean_10'],\n",
    "                yerr = aux,\n",
    "                label=['6 tpcs', '8 tpcs', '10 tpcs'], use_index=True,\n",
    "                color=color,\n",
    "                capsize=4, rot=0)\n",
    "\n",
    "        ax2 = aux_df_ds_plot.plot(\n",
    "                ax=ax, \n",
    "                y='size_mean_6', \n",
    "                kind = 'line',\n",
    "                color='#A9294F',\n",
    "                label='Nr docs', \n",
    "                secondary_y=True,\n",
    "                linewidth=3)\n",
    "\n",
    "        ax2.hlines(y=[prop_max,prop_min], xmin=-1, xmax=len(aux_df_ds_plot.thr.unique()),\n",
    "                   colors='purple', linestyles='--', lw=3,\n",
    "                   label='Nr docs max / Nr docs min')\n",
    "        ax2.set_ylim([0, 32])\n",
    "        ax.set_ylim([aux_df_ds_plot[y_aux].values.min()*(max_min[0]), aux_df_ds_plot[y_aux].values.max()*max_min[1]])\n",
    "        ax.set_xlabel('Threshold')\n",
    "        ax.set_title(f'Submodels generated from topic {tpc}')\n",
    "        ax.get_legend().remove()\n",
    "        ax.grid()\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    fig.legend(handles+handles2, labels+labels2, loc='upper center', ncol=6, bbox_to_anchor=(0.5, 1.02),\n",
    "              frameon=True, shadow=True)\n",
    "    \n",
    "    \n",
    "    fig.text(-0.01, 0.5, label, va='center', rotation='vertical')\n",
    "    fig.text(1.01, 0.5,'% of docs in the original corpus', va='center', rotation='vertical')\n",
    "    fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
