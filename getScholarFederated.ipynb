{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319e2032",
   "metadata": {},
   "source": [
    "# Attainment of Scholar subsets for training of gFedNTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a258e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ccc6a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparknlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparknlp'"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from difflib import SequenceMatcher, get_close_matches\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, explode, regexp_replace, length, lit, year, split, arrays_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370e0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16b2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_scholar_raw = Path(\"/export/usuarios_ml4ds/lbartolome/scholar/SemanticScholar/20220201/papers.parquet\")\n",
    "path_scholar_lemmas = Path(\"/export/data_ml4ds/IntelComp/Datasets/lemmatization_output/lemmatization_output_Sep_2022/output_semantic\")\n",
    "path_lemmas_federated_small = Path(\"/export/usuarios_ml4ds/lbartolome/data/scholar_federated_small\")\n",
    "path_lemmas_federated_tiny = Path(\"/export/usuarios_ml4ds/lbartolome/data/scholar_federated_tiny\")\n",
    "path_embeddings = Path(\"/export/data_ml4ds/IntelComp/Datasets/Datasets_embeddings/scholar-embeddings_v2.parquet\")\n",
    "path_output_small = Path(\"/export/usuarios_ml4ds/lbartolome/gFedNTM/aux/corpus.parquet\")\n",
    "path_output_tiny = Path(\"/export/usuarios_ml4ds/lbartolome/gFedNTM/aux_small/corpus.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ebbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lemmas_federated = path_lemmas_federated_small\n",
    "path_output = path_output_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb26387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(path_scholar_raw)\n",
    "df = df.sample(frac=0.00001, replace=True, random_state=1)\n",
    "#df2 = df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cfcd729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'paperAbstract', 's2Url', 'pdfUrls', 'year', 'sources',\n",
       "       'doi', 'doiUrl', 'pmid', 'magId', 'fieldsOfStudy', 'journalName',\n",
       "       'journalPages', 'journalVolume', 'venue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76995565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.fieldsOfStudy.str.contains('Computer Science')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7255f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['fieldsOfStudy'].str.contains(r'\\\"Computer\\sScience\"', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47de065b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Float64Index([nan], dtype='float64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/base.py:603\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    601\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 603\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m args, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys), \u001b[38;5;28mlen\u001b[39m(args)))\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/core.py:149\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, out, cache)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    148\u001b[0m     task \u001b[38;5;241m=\u001b[39m dsk[key]\n\u001b[0;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     cache[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/export/usuarios_ml4ds/lbartolome/hierarchical-topic-models/.venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Float64Index([nan], dtype='float64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df2.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b17aac",
   "metadata": {},
   "source": [
    "## 1. Get categories in Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b915a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "S2papers = spark.read.parquet(f\"file://{path_scholar_raw}\")\n",
    "\n",
    "#Concatenate text fields from which lemmas are calculated\n",
    "S2papers = (\n",
    "    S2papers.withColumn(\"text\",F.concat_ws('. ', \"title\", \"paperAbstract\"))\n",
    "    .drop(\"title\")\n",
    "    .drop(\"paperAbstract\").select(\"id\", \"year\",\"fieldsOfStudy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28481b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, year: bigint, fieldsOfStudy: array<string>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1099eb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>(991 + 9) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Scholar: 204457855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Number of papers in Scholar:', S2papers.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a76aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "years = S2papers.select('year').distinct().toPandas()\n",
    "y = sorted(years.values.tolist())#1961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e7e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Economics, Medicine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Medicine, Physics, Materials Science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Biology, Geology, Engineering]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Engineering, Psychology, Physics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Art, History, Medicine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10119</th>\n",
       "      <td>[Chemistry, Mathematics, Biology, Physics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10120</th>\n",
       "      <td>[Art, Philosophy, Computer Science, Political ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>[Art, Psychology, Biology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>[Chemistry, Business, Economics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>[Art, Medicine, Geography, Political Science]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10124 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fieldsOfStudy\n",
       "0                                  [Economics, Medicine]\n",
       "1                 [Medicine, Physics, Materials Science]\n",
       "2                        [Biology, Geology, Engineering]\n",
       "3                     [Engineering, Psychology, Physics]\n",
       "4                               [Art, History, Medicine]\n",
       "...                                                  ...\n",
       "10119         [Chemistry, Mathematics, Biology, Physics]\n",
       "10120  [Art, Philosophy, Computer Science, Political ...\n",
       "10121                         [Art, Psychology, Biology]\n",
       "10122                   [Chemistry, Business, Economics]\n",
       "10123      [Art, Medicine, Geography, Political Science]\n",
       "\n",
       "[10124 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = S2papers.select('fieldsOfStudy').distinct().toPandas()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0349c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Computer Science: 14801864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Economics: 3266129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Sociology: 5039428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Environmental Science: 3231822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:====================================================>(997 + 3) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Political Science: 7100508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "papers_s2cs = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Computer Science'))\n",
    "count_s2cs = papers_s2cs.count()\n",
    "print('Number of papers in Computer Science:', count_s2cs)\n",
    "\n",
    "papers_s2eco = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Economics'))\n",
    "count_s2eco = papers_s2eco.count()\n",
    "print('Number of papers in Economics:', count_s2eco)\n",
    "\n",
    "papers_s2socio = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Sociology'))\n",
    "count_s2socio =  papers_s2socio.count()\n",
    "print('Number of papers in Sociology:', count_s2socio)\n",
    "\n",
    "papers_s2phi = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Environmental Science'))\n",
    "count_s2phi = papers_s2phi.count()\n",
    "print('Number of papers in Environmental Science:', count_s2phi)\n",
    "\n",
    "papers_s2ps = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Political Science'))\n",
    "count_s2ps = papers_s2ps.count()\n",
    "print('Number of papers in Political Science:', count_s2ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d8566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10706f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:================================================>   (924 + 40) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Computer Science: 3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "papers_s2cs = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Computer Science')).sample(fraction=total/count_s2cs)\n",
    "print('Number of papers in Computer Science:', papers_s2cs.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1beaed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=================================================>  (948 + 40) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Economics: 2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "papers_s2eco = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Economics')).sample(fraction=total/count_s2eco)\n",
    "print('Number of papers in Economics:', papers_s2eco.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c0c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================================>      (878 + 40) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Sociology: 2963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:==================================================> (966 + 34) / 1000]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "papers_s2socio = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Sociology')).sample(fraction=total/count_s2socio)\n",
    "print('Number of papers in Sociology:', papers_s2socio.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aed60767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:================================================>   (926 + 40) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Environmental Science: 2996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "papers_s2phi = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Philosophy')).sample(fraction=total/count_s2phi)\n",
    "print('Number of papers in Philosophy:', papers_s2phi.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f5b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===================================================>(986 + 14) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Political Science: 2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "papers_s2ps = S2papers.filter(F.array_contains(\"fieldsOfStudy\", 'Political Science')).sample(fraction=total/count_s2ps)\n",
    "print('Number of papers in Political Science:', papers_s2ps.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ee1f5",
   "metadata": {},
   "source": [
    "## 2. Get lemmas of selected fields with POS [NOUN, VERB, ADJ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "740648fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------\n",
      " id          | 67a86b9076b2bfcc8fdc3054c4b6215ae4a69129                                                                                 \n",
      " embeddings  | 0.047143493. -0.4874212. 0.14710653. -0.34913865. -0.030980468. -0.6288803. 0.30476338. -0.04096819. 0.64699614. -0.5... \n",
      " lemmas_list | [spatial, dealiasing, of, 3-d, seismic, reflection, datum, SUMMARY, one, of, the, main, challenge, of, 3-d, reflectio... \n",
      " pos_list    | [ADJ, NOUN, ADP, NUM, ADJ, NOUN, NOUN, PROPN, NUM, ADP, DET, ADJ, NOUN, ADP, NUM, NOUN, NOUN, AUX, VERB, DET, ADJ, NO... \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------\n",
      " id          | 30e2c1e54a88f22513458f53129e04de590ee515                                                                                 \n",
      " embeddings  | 0.0045647793. -0.3976266. 0.30827576. -0.19942245. 0.20692979. -0.016659686. 0.16494633. -0.26790214. 0.39947075. -0.... \n",
      " lemmas_list | [low-pressure, spray, form, of, 2024, aluminum, alloy, abstract, in, this, paper, ,, a, newly, develop, low-pressure,... \n",
      " pos_list    | [NOUN, NOUN, VERB, ADP, NUM, NOUN, NOUN, ADJ, ADP, DET, NOUN, PUNCT, DET, ADV, VERB, NOUN, NOUN, VERB, PUNCT, PROPN, ... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "S2lemmas = spark.read.parquet(f\"file://{path_scholar_lemmas}\")\n",
    "\n",
    "S2lemmas = (\n",
    "    S2lemmas.withColumn(\"lemmas\",F.concat_ws(' ', \"title_lemmas\", \"paperAbstract_lemmas\"))\n",
    "    .withColumn(\"embeddings\",F.concat_ws('. ', \"title_embeddings\", \"paperAbstract_embeddings\"))\n",
    "    .withColumn(\"pos\",F.concat_ws(' ', \"title_pos\", \"paperAbstract_pos\"))\n",
    "    .withColumn(\"lemmas_list\", split(\"lemmas\", \" \"))\n",
    "    .withColumn(\"pos_list\", split(\"pos\", \" \"))\n",
    "    .select(\"id\", \"embeddings\", \"lemmas_list\", \"pos_list\")\n",
    ")\n",
    "S2lemmas = S2lemmas.where(length(col(\"lemmas\")) > 5)\n",
    "S2lemmas.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb20ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------------------------------------\n",
      " id     | 67a86b9076b2bfcc8fdc3054c4b6215ae4a69129                                                                                 \n",
      " lemmas | spatial dealiasing seismic reflection datum main challenge reflection seismology provide spatial sampling require avo... \n",
      "-RECORD 1--------------------------------------------------------------------------------------------------------------------------\n",
      " id     | 30e2c1e54a88f22513458f53129e04de590ee515                                                                                 \n",
      " lemmas | low-pressure spray form aluminum alloy abstract paper develop low-pressure spray form technique describe experimental... \n",
      "-RECORD 2--------------------------------------------------------------------------------------------------------------------------\n",
      " id     | aae5e57ccb5555bf83e3ba8a78c37d1f5cb434b2                                                                                 \n",
      " lemmas |                                                                                                                          \n",
      "-RECORD 3--------------------------------------------------------------------------------------------------------------------------\n",
      " id     | caed8cec7c9d622b333b41225a504c381ee18984                                                                                 \n",
      " lemmas | small town know many housing development recreate village life quaint shop click read other                              \n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "S2lemmas = (S2lemmas.select(\"id\", \"embeddings\", expr(\"\"\"\n",
    "                    filter(\n",
    "                    arrays_zip(lemmas_list, pos_list),\n",
    "                    e -> e.pos_list == 'ADJ' or e.pos_list == 'NOUN' or e.pos_list == 'VERB'\n",
    "                    ).lemmas_list AS result\n",
    "                    \"\"\"))\n",
    "                   .withColumn(\"lemmas\",F.concat_ws(' ', \"result\"))\n",
    "                   .select(\"id\", \"lemmas\")\n",
    "                  )\n",
    "S2lemmas.show(n=4, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fad8441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 22d91e5c3cc5c2a2d9d3a6371dd3fe45d751a612                                                                                 \n",
      " lemmas        |                                                                                                                          \n",
      " year          | 2019                                                                                                                     \n",
      " fieldsOfStudy | computer_science                                                                                                         \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 2a5d279daf91532621cd36f77047ef52d9d90568                                                                                 \n",
      " lemmas        | boolean function duality introduce new class boolean function duality hold call function consider dual notion boolean... \n",
      " year          | 2014                                                                                                                     \n",
      " fieldsOfStudy | computer_science                                                                                                         \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cs_lemmas = (S2lemmas.join(papers_s2cs, S2lemmas.id == papers_s2cs.id, \"right\")\n",
    "                     .drop(papers_s2cs.id)\n",
    "                     .withColumn(\"fieldsOfStudy\", lit(\"computer_science\"))\n",
    "            )\n",
    "cs_lemmas.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aa37af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 06dfb45d1b3fcc3de52cad5213aad677832eefe3                                                                                 \n",
      " lemmas        | first edition nominate book year second edition ask more money spend help world poor people spend use common framewor... \n",
      " year          | 2004                                                                                                                     \n",
      " fieldsOfStudy | economics                                                                                                                \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 112654979fd703dd32df9202f3315a32da75569a                                                                                 \n",
      " lemmas        | introduce reduced-form modeling framework mortgage- back security solve imply prepayment function cross section marke... \n",
      " year          | 2016                                                                                                                     \n",
      " fieldsOfStudy | economics                                                                                                                \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eco_lemmas = (S2lemmas.join(papers_s2eco, S2lemmas.id == papers_s2eco.id, \"right\")\n",
    "                     .drop(papers_s2eco.id)\n",
    "                     .withColumn(\"fieldsOfStudy\", lit(\"economics\"))\n",
    "            )\n",
    "eco_lemmas.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f51bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0cd5cf3d38a9acf8f4744ad0e5a5194983b61ba7                                                                                 \n",
      " lemmas        | political stereotype form obstacle enforcement decision article enquire present african political landscape conducive... \n",
      " year          | 2010                                                                                                                     \n",
      " fieldsOfStudy | sociology                                                                                                                \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 36730a87890bb7b4547386c42fc0aa61ee5d9b2a                                                                                 \n",
      " lemmas        | fellow information explode information technology change such bewildering speed face fundamental problem orient new l... \n",
      " year          | 2008                                                                                                                     \n",
      " fieldsOfStudy | sociology                                                                                                                \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "socio_lemmas = (S2lemmas.join(papers_s2socio, S2lemmas.id == papers_s2socio.id, \"right\")\n",
    "                     .drop(papers_s2socio.id)\n",
    "                     .withColumn(\"fieldsOfStudy\", lit(\"sociology\"))\n",
    "            )\n",
    "socio_lemmas.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1f8344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0475198e31fabce94dc5186798c5513d15139d26                                                                                 \n",
      " lemmas        | carbon nitrogen sequestration follow forage grassland conversion effect co_2 flux take sample forage grassland alfalf... \n",
      " year          | 2009                                                                                                                     \n",
      " fieldsOfStudy | environmental_science                                                                                                    \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 12cd49e0b20aa2bce9c584d2c851dfcb7c940571                                                                                 \n",
      " lemmas        | measure aboveground biomass allocation ponderosa alter andesite montane desert climate substrate hold constant climat... \n",
      " year          | 1994                                                                                                                     \n",
      " fieldsOfStudy | environmental_science                                                                                                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "phi_lemmas = (S2lemmas.join(papers_s2phi, S2lemmas.id == papers_s2phi.id, \"right\")\n",
    "                     .drop(papers_s2phi.id)\n",
    "                     .withColumn(\"fieldsOfStudy\", lit(\"philosophy\"))\n",
    "            )\n",
    "phi_lemmas.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51bf6c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0fcfb740cdc7a5fa1daf77279c1ae5175ca4b950                                                                                 \n",
      " lemmas        | 20th 21st paper investigate socio-economic role waqf 20th century date endure various political fortune improve socia... \n",
      " year          | 2017                                                                                                                     \n",
      " fieldsOfStudy | political_science                                                                                                        \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | null                                                                                                                     \n",
      " lemmas        | null                                                                                                                     \n",
      " year          | 1989                                                                                                                     \n",
      " fieldsOfStudy | political_science                                                                                                        \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ps_lemmas = (S2lemmas.join(papers_s2ps, S2lemmas.id == papers_s2ps.id, \"right\")\n",
    "                     .drop(papers_s2ps.id)\n",
    "                     .withColumn(\"fieldsOfStudy\", lit(\"political_science\"))\n",
    "            )\n",
    "ps_lemmas.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9556a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "lemmas_federated = unionAll(cs_lemmas, eco_lemmas, socio_lemmas, phi_lemmas, ps_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a031f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:(83 + 40) / 4000][Stage 48:>(0 + 0) / 1000][Stage 49:>(0 + 0) / 1000]]\r"
     ]
    }
   ],
   "source": [
    "print('Number of papers selected:', lemmas_federated.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmas_federated.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5443bb6",
   "metadata": {},
   "source": [
    "## 3. Save lemmas of five selected categories to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_federated.coalesce(1000).write.parquet(f\"file://{path_lemmas_federated}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f910a5",
   "metadata": {},
   "source": [
    "## 4. Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9f9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import pathlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "from dask.diagnostics import ProgressBar\n",
    "from gensim import corpora\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import (CountVectorizer, StopWordsRemover,\n",
    "                                Tokenizer)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Topicmodeling\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0be21d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class textPreproc(object):\n",
    "    \"\"\"\n",
    "    A simple class to carry out some simple text preprocessing tasks\n",
    "    that are needed by topic modeling\n",
    "    - Stopword removal\n",
    "    - Replace equivalent terms\n",
    "    - Calculate BoW\n",
    "    - Generate the files that are needed for training of different\n",
    "      topic modeling technologies\n",
    "\n",
    "    It allows to use Gensim or Spark functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stw_files=[], eq_files=[],\n",
    "                 min_lemas=15, no_below=10, no_above=0.6,\n",
    "                 keep_n=100000, cntVecModel=None,\n",
    "                 GensimDict=None, logger=None):\n",
    "        \"\"\"\n",
    "        Initilization Method\n",
    "        Stopwords and the dictionary of equivalences will be loaded\n",
    "        during initialization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stw_files: list of str\n",
    "            List of paths to stopwords files\n",
    "        eq_files: list of str\n",
    "            List of paths to equivalent terms files\n",
    "        min_lemas: int\n",
    "            Minimum number of lemas for document filtering\n",
    "        no_below: int\n",
    "            Minimum number of documents to keep a term in the vocabulary\n",
    "        no_above: float\n",
    "            Maximum proportion of documents to keep a term in the vocab\n",
    "        keep_n: int\n",
    "            Maximum vocabulary size\n",
    "        cntVecModel : pyspark.ml.feature.CountVectorizerModel\n",
    "            CountVectorizer Model to be used for the BOW calculation\n",
    "        GensimDict : gensim.corpora.Dictionary\n",
    "            Optimized Gensim Dictionary Object\n",
    "        logger: Logger object\n",
    "            To log object activity\n",
    "        \"\"\"\n",
    "        self._stopwords = self._loadSTW(stw_files)\n",
    "        print(self._stopwords)\n",
    "        self._equivalents = self._loadEQ(eq_files)\n",
    "        self._min_lemas = min_lemas\n",
    "        self._no_below = no_below\n",
    "        self._no_above = no_above\n",
    "        self._keep_n = keep_n\n",
    "        self._cntVecModel = cntVecModel\n",
    "        self._GensimDict = GensimDict\n",
    "\n",
    "        if logger:\n",
    "            self._logger = logger\n",
    "        else:\n",
    "            import logging\n",
    "            logging.basicConfig(level='INFO')\n",
    "            self._logger = logging.getLogger('textPreproc')\n",
    "\n",
    "    def _loadSTW(self, stw_files):\n",
    "        \"\"\"\n",
    "        Loads all stopwords from all files provided in the argument\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stw_files: list of str\n",
    "            List of paths to stopwords files\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stopWords: list of str\n",
    "            List of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        stopWords = []\n",
    "        for stwFile in stw_files:\n",
    "            with Path(stwFile).open('r', encoding='utf8') as fin:\n",
    "                stopWords += json.load(fin)['wordlist']\n",
    "\n",
    "        return list(set(stopWords))\n",
    "\n",
    "    def _loadEQ(self, eq_files):\n",
    "        \"\"\"\n",
    "        Loads all equivalent terms from all files provided in the argument\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eq_files: list of str\n",
    "            List of paths to equivalent terms files\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        equivalents: dictionary\n",
    "            Dictionary of term_to_replace -> new_term\n",
    "        \"\"\"\n",
    "\n",
    "        equivalent = {}\n",
    "\n",
    "        for eqFile in eq_files:\n",
    "            with Path(eqFile).open('r', encoding='utf8') as fin:\n",
    "                newEq = json.load(fin)['wordlist']\n",
    "            newEq = [x.split(':') for x in newEq]\n",
    "            newEq = [x for x in newEq if len(x) == 2]\n",
    "            newEq = dict(newEq)\n",
    "            equivalent = {**equivalent, **newEq}\n",
    "\n",
    "        return equivalent\n",
    "\n",
    "    def preprocBOW(self, trDF, nw=0):\n",
    "        \"\"\"\n",
    "        Preprocesses the documents in the dataframe to carry\n",
    "        out the following tasks\n",
    "            - Filter out short documents (below min_lemas)\n",
    "            - Cleaning of stopwords\n",
    "            - Equivalent terms application\n",
    "            - BoW calculation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trDF: Dask or Spark dataframe\n",
    "            This routine works on the following column \"all_lemmas\"\n",
    "            Other columns are left untouched\n",
    "        nw: Number of workers to use if Dask is selected\n",
    "            If nw=0 use Dask default value (number of cores)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        trDFnew: A new dataframe with a new colum bow containing the\n",
    "        bow representation of the documents\n",
    "        \"\"\"\n",
    "        if isinstance(trDF, dd.DataFrame):\n",
    "\n",
    "            def tkz_clean_str(rawtext):\n",
    "                \"\"\"Function to carry out tokenization and cleaning of text\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                rawtext: str\n",
    "                    string with the text to lemmatize\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                cleantxt: str\n",
    "                    Cleaned text\n",
    "                \"\"\"\n",
    "                if rawtext == None or rawtext == '':\n",
    "                    return ''\n",
    "                else:\n",
    "                    # lowercase and tokenization (similar to Spark tokenizer)\n",
    "                    cleantext = rawtext.lower().split()\n",
    "                    # remove stopwords\n",
    "                    cleantext = [\n",
    "                        el for el in cleantext if el not in self._stopwords]\n",
    "                    # replacement of equivalent words\n",
    "                    cleantext = [self._equivalents[el] if el in self._equivalents else el\n",
    "                                 for el in cleantext]\n",
    "                return cleantext\n",
    "\n",
    "            # Compute tokens, clean them, and filter out documents\n",
    "            # with less than minimum number of lemmas\n",
    "            trDF['final_tokens'] = trDF['all_lemmas'].apply(\n",
    "                tkz_clean_str, meta=('all_lemmas', 'object'))\n",
    "            trDF = trDF.loc[trDF.final_tokens.apply(\n",
    "                len, meta=('final_tokens', 'int64')) >= self._min_lemas]\n",
    "\n",
    "            # Gensim dictionary creation. It persists the created Dataframe\n",
    "            # to accelerate dictionary calculation\n",
    "            # Filtering of words is carried out according to provided values\n",
    "            self._logger.info('-- -- Gensim Dictionary Generation')\n",
    "\n",
    "            with ProgressBar():\n",
    "                DFtokens = trDF[['final_tokens']]\n",
    "                if nw>0:\n",
    "                    DFtokens = DFtokens.compute(scheduler='processes', num_workers=nw)\n",
    "                else:\n",
    "                    #Use Dask default (i.e., number of available cores)\n",
    "                    DFtokens = DFtokens.compute(scheduler='processes')\n",
    "            self._GensimDict = corpora.Dictionary(\n",
    "                DFtokens['final_tokens'].values.tolist())\n",
    "\n",
    "            # Remove words that appear in less than no_below documents, or in more than\n",
    "            # no_above, and keep at most keep_n most frequent terms\n",
    "\n",
    "            self._logger.info('-- -- Gensim Filter Extremes')\n",
    "\n",
    "            self._GensimDict.filter_extremes(no_below=self._no_below,\n",
    "                                             no_above=self._no_above, keep_n=self._keep_n)\n",
    "\n",
    "            # We skip the calculation of the bow for each document, because Spark LDA will\n",
    "            # not be used in this case. Note that this is different from what is done for\n",
    "            # Spark preprocessing\n",
    "            trDFnew = trDF\n",
    "\n",
    "        else:\n",
    "            # Preprocess data using Spark\n",
    "            # tokenization\n",
    "            tk = Tokenizer(inputCol=\"all_lemmas\", outputCol=\"tokens\")\n",
    "            trDF = tk.transform(trDF)\n",
    "\n",
    "            # Removal of Stopwords - Skip if not stopwords are provided\n",
    "            # to save computation time\n",
    "            if len(self._stopwords):\n",
    "                swr = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_tokens\",\n",
    "                                       stopWords=self._stopwords)\n",
    "                trDF = swr.transform(trDF)\n",
    "            else:\n",
    "                # We need to create a copy of the tokens with the new name\n",
    "                trDF = trDF.withColumn(\"clean_tokens\", trDF[\"tokens\"])\n",
    "\n",
    "            # Filter according to number of lemmas in each document\n",
    "            trDF = trDF.where(F.size(F.col(\"clean_tokens\")) >= self._min_lemas)\n",
    "\n",
    "            # Equivalences replacement\n",
    "            if len(self._equivalents):\n",
    "                df = trDF.select(trDF.id, F.explode(trDF.clean_tokens))\n",
    "                df = df.na.replace(self._equivalents, 1)\n",
    "                df = df.groupBy(\"id\").agg(F.collect_list(\"col\"))\n",
    "                trDF = (trDF.join(df, trDF.id == df.id, \"left\")\n",
    "                        .drop(df.id)\n",
    "                        .withColumnRenamed(\"collect_list(col)\", \"final_tokens\")\n",
    "                        )\n",
    "            else:\n",
    "                # We need to create a copy of the tokens with the new name\n",
    "                trDF = trDF.withColumn(\"final_tokens\", trDF[\"clean_tokens\"])\n",
    "\n",
    "            if not self._cntVecModel:\n",
    "                cntVec = CountVectorizer(inputCol=\"final_tokens\",\n",
    "                                         outputCol=\"bow\", minDF=self._no_below,\n",
    "                                         maxDF=self._no_above, vocabSize=self._keep_n)\n",
    "                self._cntVecModel = cntVec.fit(trDF)\n",
    "\n",
    "            trDFnew = (self._cntVecModel.transform(trDF)\n",
    "                           .drop(\"tokens\", \"clean_tokens\", \"final_tokens\")\n",
    "                       )\n",
    "\n",
    "        return trDFnew\n",
    "\n",
    "    def saveCntVecModel(self, dirpath):\n",
    "        \"\"\"\n",
    "        Saves a Count Vectorizer Model to the specified path\n",
    "        Saves also a text document with the corresponding\n",
    "        vocabulary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dirpath: pathlib.Path\n",
    "            The folder where the CountVectorizerModel and the\n",
    "            text file with the vocabulary will be saved\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        status: int\n",
    "            - 1: If the files were generated sucessfully\n",
    "            - 0: Error (Count Vectorizer Model does not exist)\n",
    "        \"\"\"\n",
    "        if self._cntVecModel:\n",
    "            cntVecModel = dirpath.joinpath('CntVecModel')\n",
    "            if cntVecModel.is_dir():\n",
    "                shutil.rmtree(cntVecModel)\n",
    "            self._cntVecModel.save(f\"file://{cntVecModel.as_posix()}\")\n",
    "            with dirpath.joinpath('vocabulary.txt').open('w', encoding='utf8') as fout:\n",
    "                fout.write(\n",
    "                    '\\n'.join([el for el in self._cntVecModel.vocabulary]))\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def saveGensimDict(self, dirpath):\n",
    "        \"\"\"\n",
    "        Saves a Gensim Dictionary to the specified path\n",
    "        Saves also a text document with the corresponding\n",
    "        vocabulary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dirpath: pathlib.Path\n",
    "            The folder where the Gensim dictionary and the\n",
    "            text file with the vocabulary will be saved\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        status: int\n",
    "            - 1: If the files were generated sucessfully\n",
    "            - 0: Error (Gensim dictionary does not exist)\n",
    "        \"\"\"\n",
    "        if self._GensimDict:\n",
    "            GensimFile = dirpath.joinpath('dictionary.gensim')\n",
    "            if GensimFile.is_file():\n",
    "                GensimFile.unlink()\n",
    "            self._GensimDict.save_as_text(GensimFile)\n",
    "            with dirpath.joinpath('vocabulary.txt').open('w', encoding='utf8') as fout:\n",
    "                fout.write(\n",
    "                    '\\n'.join([self._GensimDict[idx] for idx in range(len(self._GensimDict))]))\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def exportTrData(self, trDF, dirpath, tmTrainer, nw=0):\n",
    "        \"\"\"\n",
    "        Exports the training data in the provided dataset to the\n",
    "        format required by the topic modeling trainer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trDF: Dask or Spark dataframe\n",
    "            If Spark, the dataframe should contain a column \"bow\" that will\n",
    "            be used to calculate the training data\n",
    "            If Dask, it should contain a column \"final_tokens\"\n",
    "        dirpath: pathlib.Path\n",
    "            The folder where the data will be saved\n",
    "        tmTrainer: string\n",
    "            The output format [mallet|sparkLDA|prodLDA|ctm]\n",
    "        nw: Number of workers to use if Dask is selected\n",
    "            If nw=0 use Dask default value (number of cores)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outFile: Path\n",
    "            A path containing the location of the training data in the indicated format\n",
    "        \"\"\"\n",
    "\n",
    "        self._logger.info(f'-- -- Exporting corpus to {tmTrainer} format')\n",
    "\n",
    "        if isinstance(trDF, dd.DataFrame):\n",
    "            # Dask dataframe\n",
    "\n",
    "            # Remove words not in dictionary, and return a string\n",
    "            vocabulary = set([self._GensimDict[idx]\n",
    "                             for idx in range(len(self._GensimDict))])\n",
    "\n",
    "            def tk_2_text(tokens):\n",
    "                \"\"\"Function to filter words not in dictionary, and\n",
    "                return a string of lemmas \n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                tokens: list\n",
    "                    list of \"final_tokens\"\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                lemmasstr: str\n",
    "                    Clean text including only the lemmas in the dictionary\n",
    "                \"\"\"\n",
    "                #bow = self._GensimDict.doc2bow(tokens)\n",
    "                # return ''.join([el[1] * (self._GensimDict[el[0]]+ ' ') for el in bow])\n",
    "                return ' '.join([el for el in tokens if el in vocabulary])\n",
    "\n",
    "            trDF['cleantext'] = trDF['final_tokens'].apply(\n",
    "                tk_2_text, meta=('final_tokens', 'str'))\n",
    "\n",
    "            if tmTrainer == \"mallet\":\n",
    "\n",
    "                outFile = dirpath.joinpath('corpus.txt')\n",
    "                if outFile.is_file():\n",
    "                    outFile.unlink()\n",
    "\n",
    "                trDF['2mallet'] = trDF['id'].apply(\n",
    "                    str, meta=('id', 'str')) + \" 0 \" + trDF['cleantext']\n",
    "\n",
    "                with ProgressBar():\n",
    "                    #trDF = trDF.persist(scheduler='processes')\n",
    "                    DFmallet = trDF[['2mallet']]\n",
    "                    if nw>0:\n",
    "                        DFmallet.to_csv(outFile, index=False, header=False, single_file=True,\n",
    "                                    compute_kwargs={'scheduler': 'processes', 'num_workers': nw})\n",
    "                    else:\n",
    "                        #Use Dask default number of workers (i.e., number of cores)\n",
    "                        DFmallet.to_csv(outFile, index=False, header=False, single_file=True,\n",
    "                                    compute_kwargs={'scheduler': 'processes'})\n",
    "\n",
    "            elif tmTrainer == 'sparkLDA':\n",
    "                self._logger.error(\n",
    "                    '-- -- sparkLDA requires preprocessing with spark')\n",
    "                return\n",
    "\n",
    "            elif tmTrainer == \"prodLDA\":\n",
    "\n",
    "                outFile = dirpath.joinpath('corpus.parquet')\n",
    "                if outFile.is_file():\n",
    "                    outFile.unlink()\n",
    "\n",
    "                with ProgressBar():\n",
    "                    DFparquet = trDF[['id', 'cleantext']].rename(\n",
    "                        columns={\"cleantext\": \"bow_text\"})\n",
    "                    if nw>0:\n",
    "                        DFparquet.to_parquet(outFile, write_index=False, compute_kwargs={\n",
    "                                         'scheduler': 'processes', 'num_workers': nw})\n",
    "                    else:\n",
    "                        #Use Dask default number of workers (i.e., number of cores)\n",
    "                        DFparquet.to_parquet(outFile, write_index=False, compute_kwargs={\n",
    "                                         'scheduler': 'processes'})\n",
    "\n",
    "            elif tmTrainer == \"ctm\":\n",
    "                outFile = dirpath.joinpath('corpus.parquet')\n",
    "                if outFile.is_file():\n",
    "                    outFile.unlink()\n",
    "\n",
    "                with ProgressBar():\n",
    "                    # DFparquet = trDF[['id', 'cleantext', 'all_rawtext']].rename(\n",
    "                    #    columns={\"cleantext\": \"bow_text\"})\n",
    "                    DFparquet = trDF[['id', 'cleantext', 'embeddings']].rename(\n",
    "                        columns={\"cleantext\": \"bow_text\"})\n",
    "                    schema = pa.schema([\n",
    "                        ('id', pa.int64()),\n",
    "                        ('bow_text', pa.string()),\n",
    "                        ('embeddings', pa.list_(pa.float64()))\n",
    "                    ])\n",
    "                    if nw>0:\n",
    "                        DFparquet.to_parquet(outFile, write_index=False, schema=schema, compute_kwargs={\n",
    "                                         'scheduler': 'processes', 'num_workers': nw})\n",
    "                    else:\n",
    "                        #Use Dask default number of workers (i.e., number of cores)\n",
    "                        DFparquet.to_parquet(outFile, write_index=False, schema=schema, compute_kwargs={\n",
    "                                         'scheduler': 'processes'})\n",
    "\n",
    "        else:\n",
    "            # Spark dataframe\n",
    "            if tmTrainer == \"mallet\":\n",
    "                # We need to convert the bow back to text, and save text file\n",
    "                # in mallet format\n",
    "                outFile = dirpath.joinpath('corpus.txt')\n",
    "                vocabulary = self._cntVecModel.vocabulary\n",
    "                spark.sparkContext.broadcast(vocabulary)\n",
    "\n",
    "                # User defined function to recover the text corresponding to BOW\n",
    "                def back2text(bow):\n",
    "                    text = \"\"\n",
    "                    for idx, tf in zip(bow.indices, bow.values):\n",
    "                        text += int(tf) * (vocabulary[idx] + ' ')\n",
    "                    return text.strip()\n",
    "                back2textUDF = F.udf(lambda z: back2text(z))\n",
    "\n",
    "                malletDF = (trDF.withColumn(\"bow_text\", back2textUDF(F.col(\"bow\")))\n",
    "                            .withColumn(\"2mallet\", F.concat_ws(\" 0 \", \"id\", \"bow_text\"))\n",
    "                            .select(\"2mallet\")\n",
    "                            )\n",
    "                # Save as text file\n",
    "                # Ideally everything should get written to one text file directly from Spark\n",
    "                # but this is failing repeatedly, so I avoid coalescing in Spark and\n",
    "                # instead concatenate all files after creation\n",
    "                tempFolder = dirpath.joinpath('tempFolder')\n",
    "                #malletDF.coalesce(1).write.format(\"text\").option(\"header\", \"false\").save(f\"file://{tempFolder.as_posix()}\")\n",
    "                malletDF.write.format(\"text\").option(\"header\", \"false\").save(\n",
    "                    f\"file://{tempFolder.as_posix()}\")\n",
    "                # Concatenate all text files\n",
    "                with outFile.open(\"w\", encoding=\"utf8\") as fout:\n",
    "                    for inFile in [f for f in tempFolder.iterdir() if f.name.endswith('.txt')]:\n",
    "                        fout.write(inFile.open(\"r\").read())\n",
    "                shutil.rmtree(tempFolder)\n",
    "\n",
    "            elif tmTrainer == \"sparkLDA\":\n",
    "                # Save necessary columns for Spark LDA in parquet file\n",
    "                outFile = dirpath.joinpath('corpus.parquet')\n",
    "                trDF.select(\"id\", \"source\", \"bow\").write.parquet(\n",
    "                    f\"file://{outFile.as_posix()}\", mode=\"overwrite\")\n",
    "            elif tmTrainer == \"prodLDA\":\n",
    "                outFile = dirpath.joinpath('corpus.parquet')\n",
    "                lemas_df = (trDF.withColumn(\"bow_text\", back2textUDF(\n",
    "                    F.col(\"bow\"))).select(\"id\", \"bow_text\"))\n",
    "                lemas_df.write.parquet(\n",
    "                    f\"file://{outFile.as_posix()}\", mode=\"overwrite\")\n",
    "            elif tmTrainer == \"ctm\":\n",
    "                outFile = dirpath.joinpath('corpus.parquet')\n",
    "                \n",
    "                vocabulary = self._cntVecModel.vocabulary\n",
    "                spark.sparkContext.broadcast(vocabulary)\n",
    "\n",
    "                # User defined function to recover the text corresponding to BOW\n",
    "                def back2text(bow):\n",
    "                    text = \"\"\n",
    "                    for idx, tf in zip(bow.indices, bow.values):\n",
    "                        text += int(tf) * (vocabulary[idx] + ' ')\n",
    "                    return text.strip()\n",
    "                back2textUDF = F.udf(lambda z: back2text(z))\n",
    "                \n",
    "                \n",
    "                lemas_raw_df = (trDF.withColumn(\"bow_text\", back2textUDF(\n",
    "                    F.col(\"bow\"))).select(\"id\", \"bow_text\", \"embeddings\", \"fieldsOfStudy\"))\n",
    "                lemas_raw_df.write.parquet(\n",
    "                    f\"file://{outFile.as_posix()}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07123a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"TrDtSet\": \"/export/usuarios_ml4ds/lbartolome/gFedNTM/aux/datasets/scholar_dtst.json\",\n",
    "    \"Preproc\": {\n",
    "      \"min_lemas\": 15,\n",
    "      \"no_below\": 10,\n",
    "      \"no_above\": 0.6,\n",
    "      \"keep_n\": 500000,\n",
    "      \"stopwords\": [\n",
    "        \"/export/usuarios_ml4ds/lbartolome/gFedNTM/aux/wordlists/S2_stopwords.json\"\n",
    "      ],\n",
    "      \"equivalences\": [\n",
    "        \"/export/usuarios_ml4ds/lbartolome/gFedNTM/aux/wordlists/S2_equivalences.json\"\n",
    "      ]\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0a44f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eeective', '1a', 'eld', 'eectiveness', 'afford', 'ambitious', 'ea', 'duce', 'increased', 'definition', 'community', 'dence', 'arrange', 'arti', 'advance', 'chap', 'eeort', 'dia', 'elaborate', 'da', 'confi', 'cla', 'diff', 'description', 'constituent', 'er', 'det', 'compatibility', 'anal', 'designer', 'purposethe', 'work', 'article', 'confine', 'appraisal', 'design', 'dv', 'world', 'existent', 'exemplify', 'doc', 'av', 'beneficial', 'aspect', 'new', 'ba', 'upcoming', 'contribution', 'amendment', 'ence', 'ecp', 'choose', 'information', 'conclusion', 'system', 'behavior', 'emphasise', 'adequacy', 'a_number', 'inner', 'ective', 'challenging', 'def', 'separated', 'consequent', 'download', 'bl', 'cent', 'eration', 'appendix', 'cate', 'entail', 'bd', 'di', 'closer', 'categorise', 'deem', 'cre', 'ci', 'decade', 'numerous', 'demon', 'characteristic', 'coe', 'basics', 'deter', 'device', 'defi', '3rd', 'difficulty', 'dr', 'attempt', 'behaviour', 'cha', 'ect', 'dat', 'bm', 'dierence', 'comprehensive', 'complicate', 'distant', 'benet', 'diverse', 'duction', 'ain', 'att', 'devote', 'applied', 'est', 'complex', 'academia', 'adequate', 'exibility', '5b', 'comm', 'disparate', 'let', 'exist', 'depict', 'applicability', 'confront', 'ce', 'acc', 'date', 'case_study', 'empirical', 'thank', 'ess', 'need', 'ancilla', 'area', 'computa', 'culty', 'say', 'en', 'clearer', 'conguration', 'study', 'continuation', 'crease', 'diierent', 'ad', 'categorisation', 'dif', 'ec', 'enumerate', 'ate', 'choice', 'cial', 'cient', '4b', 'claim', 'eort', 'commu', 'challenge', 'criterion', 'aim', 'bachelor', 'eeect', 'comply', 'broader', 'essay', 'ascertain', 'decay', 'dha', 'eect', 'drawback', 'consolidated', 'ep', 'advanced', 'exceed', 'dispute', 'controversial', 'au', 'chal', 'conclude', 'eval', '4a', 'development', 'copy', 'unit', 'catalogue', 'ere', 'encouraging', 'discussion', '3c', 'engineering_design', 'complicated', '4c', 'eective', 'current', 'emphasis', 'devel', 'erent', 'appropriateness', 'collapse', 'dist', 'include', 'cle', 'developer', 'digital_forensics', 'analyst', 'evidence-based', 'drs', 'digital_evidence', 'ana', 'condi', 'afterglow', 'exemplary', 'civil', 'docu', 'attain', 'basis', 'account', 'aps', 'distinctive', 'betw', 'background', 'attract', 'compu', 'cm', 'eb', 'model', 'examine', 'cy', 'diierence', 'brie', 'ever-increasing', 'volume', 'suggestion', 'method', 'issue', 'module', 'ab', 'conceptual_framework', 'eve', 'characterisation', 'circumvent', 'differentiate', 'use', 'arbitrary', 'cont', 'abundance', 'introduction', 'eff', 'time', 'analytical_methods', 'conference', 'cloning', 'consequence', 'research', 'cur', 'ding', 'ets', 'self', 'sich', 'print', 'cation', 'access', 'applicable', 'definitive', 'effort', 'cp', 'case', 'controversy', 'aforementione', 'eecient', 'advent', 'elusive', 'domain', 'exhaustive', 'comp', 'exper', 'deficiency', 'ds', 'alternative', 'arise', 'established', 'application', 'diploma', 'term', 'astrometric', 'bring', 'cf', 'predictable', 'resultsthe', 'discipline', 'benefit', 'emphasize', 'emerge', 'his', 'contradictory', 'cal', 'apply', 'convincing', 'dierent', 'effec', 'describe', 'bine', 'dent', 'comparison', 'encounter', 'datum', 'environ', 'denition', 'exclude', 'commun', 'complication', 'eac', 'ection', 'ck', 'absence', 'behave', 'commonality', 'basic', 'contr', 'do', 'consolidate', 'cos', 'difierent', 'criminal', '2b', '5c', 'create', 'cen', 'ard', 'bc', 'contrary', 'classus', 'et', 'bridge', 'dp', 'ad_hoc', 'chapter', 'bu', 'clear', 'provide', 'entangled', 'registered', 'critical', 'business', 'regulation', 'document', 'characterise', 'draft', 'acs', 'advantage', '1d', 'col', 'communus', 'bosonic', 'denote', 'advantageous', 'ecient', 'diffi', 'diicult', 'excess', 'continued', 'adoption', 'exp', 'ef', 'cess', 'arc', 'cite', 'benchmarking', 'expe', 'algori', 'application_domain', 'demand', 'cessing', '3b', 'ele', 'ae', 'ass', 'een', 'above-mentioned', 'consid', 'dm', 'ent', 'cuss', 'ax', 'applica', 'contribute', 'disadvantage', 'propose', 'accretion', 'dissertation', 'bal', 'ace', 'building_blocks', 'computer_science', 'cornerstone', 'way', 'dy', 'contextuality', 'concerned', 'conduct', 'applus', 'deliverable', 'bunch', 'sense', 'ew', 'conf', 'body', 'con', 'em', 'ction', 'agn', 'discrepancy', 'achievement', 'early_stages', 'ery', 'analysis', 'exible', 'appealing', 'auto', 'ectiveness', '2a', 'bility', 'cpa', 'ambiguous', 'economical', 'erence', 'citation', 'suited', 'bi', 'afternoon', 'detail', 'book', 'char', 'later', 'classical', 'complementary', 'descr', 'author', 'comparative', 'charac', 'alleviate', 'current_literature', 'chine', 'dir', 'address', 'context', 'arena', 'dard', 'suggest', 'dd', 'example', 'clude', 'cup', 'ap', 'ancillary', 'ed', 'concern', 'dene', 'ak', 'algo', 'dictate', 'bene', 'ase', 'cept', 'elaboration', 'academic', 'define', 'allow', 'entangle', 'dn', 'classification_scheme', 'compelling', 'ach', 'difficult', 'deen', 'exclusion', 'most', 'communica', 'confusion', 'begin', '5a', 'classi', 'cover', 'early', 'distribu', 'alg', 'existing', 'focus', 'essential', 'es', 'ave', 'base', 'appl', 'erg', 'eat', 'envision', 'effi', 'broad', 'require', 'dsd', '1c', 'point', 'eral', 'consideration', 'definite', 'deal', 'ement', 'advancement', 'compo', 'binary', 'cult', 'cx', 'avenue', 'endeavour', 'ation', 'assumption', 'devise', 'algorith', '3a', 'appli', 'ble', 'dj', 'bo', 'comput', 'counterpart', 'dis', 'continue', 'result', 'ciency', '1b', 'bibliography', 'der', 'aw', 'continuum', 'bright', 'checklist', 'cus', 'delve', 'ance', 'ebs', 'emerging_technologies', 'eq', 'uence', 'bt', 'aforementioned', 'make', 'al', 'analy', 'desirable', 'approach', 'delineate', 'ata', 'cc', 'const', 'give', 'bit', 'client', 'direction', 'user', 'court', 'section', 'considerable', 'dom', 'coverage', 'el', '2c', 'comment', 'deepen', 'dot', 'ay', 'human', 'envisage', 'design_methods', 'assume', 'discuss', 'evident', 'plan', 'form', 'ear', 'debate', 'view', 'cz', 'decisive', 'dt', 'existe', 'bh', 'epr', 'ally', 'common', 'bet', 'eventual', 'detailed', 'demerit', 'den', 'ethod', 'eciency', 'cl', 'findingsthe', 'ape', 'broaden', 'agement', 'paper', 'elucidate', 'dicult', 'clarify', 'case_studies', 'clarification', 'discord', 'art', 'abstract', 'acceptance', 'cor', 'eng', 'bv', 'ei', 'directive', 'cer', 'dec', 'cn', 'event', 'classier', 'cv', 'ao', 'develop', 'architec', 'critical_review', 'analogous', 'acceptable', 'mean', 'aa', 'bb', 'age', 'briefly', 'dev', 'cb', 'encompass', 'corre', 'high']\n"
     ]
    }
   ],
   "source": [
    "tPreproc = textPreproc(\n",
    "    stw_files=train_config['Preproc']['stopwords'],\n",
    "    eq_files=train_config['Preproc']['equivalences'],\n",
    "    min_lemas=train_config['Preproc']['min_lemas'],\n",
    "    no_below=train_config['Preproc']['no_below'],\n",
    "    no_above=train_config['Preproc']['no_above'],\n",
    "    keep_n=train_config['Preproc']['keep_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5cd475e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Scholar',\n",
       " 'description': 'Topic modeling training dataset of a five subsets of Scholar',\n",
       " 'valid_for': 'TM',\n",
       " 'visibility': 'private',\n",
       " 'Dtsets': [{'parquet': '/export/usuarios_ml4ds/lbartolome/data/scholar_federated_small',\n",
       "   'source': 'Scholar',\n",
       "   'idfld': 'id',\n",
       "   'lemmasfld': ['lemmas'],\n",
       "   'filter': ''}],\n",
       " 'creation_date': '2022-09-17 17:31:13.601336'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Dataframe with all training data\n",
    "trDtFile = Path(train_config['TrDtSet'])\n",
    "with trDtFile.open() as fin:\n",
    "    trDtSet = json.load(fin)\n",
    "trDtSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da50add",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, DtSet in enumerate(trDtSet['Dtsets']):\n",
    "    df = spark.read.parquet(f\"file://{DtSet['parquet']}\")\n",
    "    if len(DtSet['filter']):\n",
    "        pass\n",
    "    df = (\n",
    "        df.withColumn(\"all_lemmas\", F.concat_ws(\n",
    "            ' ', *DtSet['lemmasfld']))\n",
    "            .withColumn(\"source\", F.lit(DtSet[\"source\"]))\n",
    "            .select(\"id\", \"source\", \"all_lemmas\")\n",
    "    )\n",
    "    if idx == 0:\n",
    "        trDF = df\n",
    "    else:\n",
    "        trDF = trDF.union(df).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b385f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-3.1.1-bin-2.8.3/python/pyspark/sql/dataframe.py:2213: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n",
      "  warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trDF = tPreproc.preprocBOW(trDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e386c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/22 18:33:52 WARN scheduler.TaskSetManager: Stage 54 contains a task of very large size (2073 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tPreproc.saveCntVecModel(Path(\"/export/usuarios_ml4ds/lbartolome/gFedNTM/aux\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f44d51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, DtSet in enumerate(trDtSet['Dtsets']):\n",
    "df = spark.read.parquet(f\"file://{path_embeddings}\")\n",
    "df = df.select(\"id\", \"embeddings\", \"fieldsOfStudy\")\n",
    "eDF = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7da189bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trDF = (trDF.join(eDF, trDF.id == eDF.id, \"left\")\n",
    "                    .drop(eDF.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa19aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trDF.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ba559b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Computer Science: 732039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Economics: 616261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Sociology: 440139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Environmental Science: 132545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Political Science: 304195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "papers_s2cs = trDF.filter(F.array_contains(\"fieldsOfStudy\", 'Computer Science')).withColumn(\"fieldsOfStudy\", lit(\"computer_science\"))\n",
    "count_s2cs = papers_s2cs.count()\n",
    "print('Number of papers in Computer Science:', count_s2cs)\n",
    "\n",
    "papers_s2eco = trDF.filter(F.array_contains(\"fieldsOfStudy\", 'Economics')).withColumn(\"fieldsOfStudy\", lit(\"economics\"))\n",
    "count_s2eco = papers_s2eco.count()\n",
    "print('Number of papers in Economics:', count_s2eco)\n",
    "\n",
    "papers_s2socio = trDF.filter(F.array_contains(\"fieldsOfStudy\", 'Sociology')).withColumn(\"fieldsOfStudy\", lit(\"sociology\"))\n",
    "count_s2socio =  papers_s2socio.count()\n",
    "print('Number of papers in Sociology:', count_s2socio)\n",
    "\n",
    "papers_s2phi = trDF.filter(F.array_contains(\"fieldsOfStudy\", 'Philosophy')).withColumn(\"fieldsOfStudy\", lit(\"philosophy\"))\n",
    "count_s2phi = papers_s2phi.count()\n",
    "print('Number of papers in Environmental Science:', count_s2phi)\n",
    "\n",
    "papers_s2ps = trDF.filter(F.array_contains(\"fieldsOfStudy\", 'Political Science')).withColumn(\"fieldsOfStudy\", lit(\"political_science\"))\n",
    "count_s2ps = papers_s2ps.count()\n",
    "print('Number of papers in Political Science:', count_s2ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fe75436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "trDF = unionAll(papers_s2cs, papers_s2eco, papers_s2socio, papers_s2phi, papers_s2ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0af07536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:textPreproc:-- -- Exporting corpus to ctm format\n",
      "22/10/22 23:11:24 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1662.6 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trDataFile = tPreproc.exportTrData(trDF=trDF,\n",
    "                                   dirpath=Path(\"/export/usuarios_ml4ds/lbartolome/gFedNTM/aux\"),\n",
    "                                   tmTrainer='ctm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "498a7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 91:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0008e1e1fafabd8e5c7418d5cbc74ace257bde81                                                                                 \n",
      " bow_text      | order first field detection detection second medium medium medium medium target target transmission associate one one... \n",
      " embeddings    | [0.03201526030898094, 0.017998920753598213, 0.04557495936751366, -0.06096094474196434, 0.006882247515022755, -0.04495... \n",
      " fieldsOfStudy | computer_science                                                                                                         \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000c9d65ac49b9a32235ab8b8038720f80f58112                                                                                 \n",
      " bow_text      | present problem process process control control control performance cost part feature % environment environment energ... \n",
      " embeddings    | [-0.0015829851618036628, -0.05368629842996597, 0.011114525608718395, 0.02803046815097332, 0.004267534706741571, -0.00... \n",
      " fieldsOfStudy | computer_science                                                                                                         \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lemmas = spark.read.parquet(f\"file://{path_output}\")\n",
    "lemmas.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f723279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:====================================================> (114 + 4) / 118]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 2225179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 00:53:56,044:33340(0x7f9a7d7dc6c0):ZOO_WARN@zookeeper_interest@1597: Exceeded deadline by 41ms\n"
     ]
    }
   ],
   "source": [
    "print('Number of papers:', lemmas.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
